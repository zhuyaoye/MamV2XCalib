# Dataset Preparation

## Short-term Video Understanding

We follow [UniFormerV2](https://github.com/OpenGVLab/UniFormerV2/) to prepare the datasets. All the files can be found [here](https://drive.google.com/drive/folders/17VB-XdF3Kfr9ORmnGyXCxTMs86n0L4QL?usp=sharing), including:
- [Kinetics-400](https://www.deepmind.com/open-source/kinetics)
- [Something-Something V2](https://developer.qualcomm.com/software/ai-datasets/something-something)

> Since some videos in Kinetics may no longer be available, it will lead to small performance gap. Our version of Kinetics-400 can be downloaded via **[Baidu Cloud](https://pan.baidu.com/s/150AE6OK9GjWQQvXv_db8vw) (password: li0f)**


## Long-term Video Understanding

We follow [ViS4mer](https://github.com/md-mohaiminul/ViS4mer) to prepare the datasets. Add the files can be found [here](https://github.com/md-mohaiminul/ViS4mer/tree/main/data), including:
- [Breakfast](https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/)
- [COIN](https://coin-dataset.github.io/)
- [LVU](https://github.com/chaoyuaw/lvu)

> We simply use the raw videos, instead of extracting features.